{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96495c3b",
   "metadata": {},
   "source": [
    "Excellent question. Let's break this down.\n",
    "\n",
    "**The short answer is: YES, absolutely.**\n",
    "\n",
    "This is a fantastic project for landing a Data Scientist or Machine Learning Engineer job. The method you've outlined is modern, relevant, and demonstrates a highly sought-after combination of skills.\n",
    "\n",
    "However, simply *completing* the steps isn't enough. It's about *how* you execute, document, and present the project. Here's a detailed breakdown of why this project is so valuable and how to leverage it to get a job.\n",
    "\n",
    "---\n",
    "\n",
    "### Why This is a Gold-Standard Portfolio Project\n",
    "\n",
    "This project hits a \"sweet spot\" for employers because it demonstrates proficiency in several key areas:\n",
    "\n",
    "1.  **Solves a Real Business Problem:** Every company deals with invoices, receipts, or forms. Automating data entry saves time and money. You can immediately explain the business value of your work.\n",
    "2.  **Multimodality (CV + NLP):** You're not just working with text or images; you're combining them. This is a cutting-edge area of AI, and LayoutLM is the perfect model to showcase this skill.\n",
    "3.  **End-to-End MLOps Thinking:** Your pipeline covers the entire machine learning lifecycle: data collection, preprocessing, model training, and inference. This is far more impressive than just training a model on a clean CSV file.\n",
    "4.  **Complex Data Preprocessing:** Step 3 (Aligning OCR with JSON) is non-trivial. This is where most \"Kaggle-only\" candidates fail. Demonstrating that you can handle messy, real-world data and build a robust data pipeline is a massive plus.\n",
    "5.  **Use of Modern Tooling:** You're using Hugging Face Transformers, which is the industry standard for NLP and related tasks. This shows you're current with the best tools for the job.\n",
    "\n",
    "---\n",
    "\n",
    "### How to Elevate Your Project from \"Good\" to \"Job-Winning\"\n",
    "\n",
    "Your proposed steps are a great starting point. To make this project truly stand out, you need to go deeper in each step and add a few more.\n",
    "\n",
    "#### **Step 1: Prepare Data (The Foundation)**\n",
    "*   **Where did you get the data?** Don't just assume you have it.\n",
    "    *   **Option A (Good):** Use a public dataset like **SROIE** or **CORD**. This is a great start.\n",
    "    *   **Option B (Better):** Use a public dataset *and* supplement it with 50-100 invoices you find online (use Google Images). This shows initiative and proves your model can generalize beyond a single, clean dataset.\n",
    "    *   **Option C (Best):** Do Option B, and also create a small set of **\"hard cases\"**: rotated images, blurry photos, invoices with coffee stains, or unusual layouts. Show that you've thought about edge cases.\n",
    "\n",
    "#### **Step 2: Run OCR (The Technical Choice)**\n",
    "*   **Justify your choice.** Don't just say \"I used Tesseract.\" Explain *why*.\n",
    "    *   \"I started with **Tesseract** because it's open-source and effective for high-quality scans. I wrote pre-processing scripts using OpenCV to deskew and binarize images, which improved Tesseract's accuracy by 15%.\"\n",
    "    *   \"For lower-quality images, I integrated the **Azure Read API** to demonstrate my ability to work with cloud services and achieve higher accuracy, at the cost of API calls.\"\n",
    "\n",
    "#### **Step 3: Align OCR Output (The \"Secret Sauce\")**\n",
    "This is your chance to shine. Document this part meticulously in your project's `README.md`.\n",
    "*   **Don't just say you \"linked\" them.** Explain your algorithm.\n",
    "    *   **Heuristics:** \"I developed a set of rules. For example, to find the `invoice_number`, I used a regex `(INV|#|Invoice No\\.)\\s?([A-Z0-9\\-]+)`. For fields like `total`, I searched for keywords like 'Total' or 'Amount Due' and then found the nearest numerical value within a certain bounding box proximity.\"\n",
    "    *   **Manual Labeling Tool:** \"For complex cases where heuristics failed, I used **Label Studio** to manually annotate the data. This demonstrates my understanding that a perfect automated pipeline is unrealistic and human-in-the-loop systems are often necessary.\"\n",
    "\n",
    "#### **Step 4: Fine-tune LayoutLMv2 (The Core ML)**\n",
    "*   **Go beyond the basics.**\n",
    "    *   **Explain the \"Why\":** \"I chose LayoutLMv2 over a simple text-based NER model because it leverages spatial information. The relative position of 'Date:' and '2024-07-08' is crucial, and LayoutLMv2 is designed to understand this.\"\n",
    "    *   **Show your work:** You're treating this as a **Token Classification** (NER) task. Explain the `B-I-O` tagging scheme you used (e.g., `B-TOTAL`, `I-TOTAL`, `O` for other words).\n",
    "    *   **Metrics:** Track **Precision, Recall, and F1-score for each entity type** (e.g., `total`, `date`, `vendor`). An overall accuracy score is not very informative. Create a classification report.\n",
    "\n",
    "#### **Step 5: Inference Pipeline (Making it Real)**\n",
    "*   **Build an API:** This is critical. A Jupyter Notebook is for experimentation; an API is for production.\n",
    "    *   Use **FastAPI** or **Flask** to wrap your inference logic. Create a simple endpoint `/parse_invoice` that accepts an image and returns the structured JSON.\n",
    "    *   **Containerize it with Docker.** This shows you understand deployment and reproducibility. It’s a huge skill for an ML Engineer.\n",
    "\n",
    "---\n",
    "\n",
    "### The Extra Mile: What Gets You Hired\n",
    "\n",
    "1.  **Create a Live Demo:** Use **Hugging Face Spaces** or Streamlit Cloud to host a simple web app. Let recruiters upload an invoice and see the results live. This is incredibly powerful.\n",
    "2.  **Detailed `README.md` on GitHub:** Your GitHub repository is your new resume. It should include:\n",
    "    *   A clear project description and its business value.\n",
    "    *   Setup and installation instructions (`requirements.txt`, Docker commands).\n",
    "    *   A \"Project Structure\" section.\n",
    "    *   A \"Methodology\" section detailing your choices (OCR, labeling, model).\n",
    "    *   A \"Results\" section with your F1 scores and examples of good and bad predictions.\n",
    "    *   A \"Future Work\" section (e.g., \"Next, I would experiment with LayoutLMv3's question-answering approach to handle unseen fields\").\n",
    "3.  **Write a Blog Post:** A Medium or personal blog post explaining your journey, challenges, and learnings is a great way to demonstrate your communication skills and establish yourself as an expert.\n",
    "\n",
    "### Tailoring Your Story for Different Roles\n",
    "\n",
    "*   **For a Data Scientist role:** Focus on the **business impact, data analysis, and model evaluation**. \"My model achieved a 95% F1-score on extracting the 'total amount', which could reduce manual data entry errors by X% and save Y hours per week.\"\n",
    "*   **For a Machine Learning Engineer role:** Focus on the **end-to-end pipeline, automation, deployment, and scalability**. \"I built a production-ready inference pipeline using FastAPI and Docker, capable of processing invoices with a P95 latency of 500ms.\"\n",
    "\n",
    "**Conclusion:** Yes, the method you outlined is the backbone of a project that can absolutely land you a top-tier job. The key is to execute it with depth, document your choices, and build a compelling narrative around it that showcases not just your coding ability, but your problem-solving and engineering mindset. Good luck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1258b587",
   "metadata": {},
   "source": [
    "Of course. Let's dive deep into **Step 1: Prepare Data**.\n",
    "\n",
    "This is arguably the most critical step in any machine learning project. The principle of \"Garbage In, Garbage Out\" is especially true here. Your model's performance is capped by the quality and quantity of your data.\n",
    "\n",
    "The goal of this step is to create a perfectly aligned dataset where for every single invoice image, you have a corresponding \"ground truth\" JSON file containing the exact text for the key fields you want to extract.\n",
    "\n",
    "Let's address your questions directly.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Augmenting Your SROIE Dataset with Google Images\n",
    "\n",
    "You have 1000 images from the SROIE dataset. This is a good start, but it's a single, well-defined dataset. Models trained only on SROIE can become very good at parsing SROIE-like receipts but may fail on invoices with different layouts. Adding variety is crucial for building a robust, real-world model.\n",
    "\n",
    "Here is a detailed, step-by-step guide to do this effectively:\n",
    "\n",
    "**Step 1.1: Search and Collect Diverse Images**\n",
    "\n",
    "Your goal is to find images that look *different* from the SROIE receipts.\n",
    "\n",
    "*   **Search Terms:** Use a variety of search terms on Google Images:\n",
    "    *   \"invoice example\"\n",
    "    *   \"receipt template\"\n",
    "    *   \"sample service invoice\"\n",
    "    *   \"purchase order example\"\n",
    "    *   \"utility bill US\"\n",
    "    *   \"restaurant bill\"\n",
    "*   **Look for Diversity:** As you save images, consciously try to find variety in:\n",
    "    *   **Layout:** Some have the total on the right, some on the bottom. Some have logos, some don't.\n",
    "    *   **Format:** Look for clean scans, but also save some real-world photos (images taken with a phone, maybe slightly skewed or with shadows).\n",
    "    *   **Type:** Collect service invoices, product receipts, travel expense reports, etc.\n",
    "    *   **Quality:** Aim for mostly clear images, but having 10-20% that are slightly blurry or have artifacts will make your model more robust.\n",
    "\n",
    "**Action:** Aim to collect around **150-250 new images**. This is a manageable number to label manually and significantly increases your dataset's diversity.\n",
    "\n",
    "**Step 1.2: Organize Your Raw Data**\n",
    "\n",
    "A clean project structure is essential. Before you do anything else, organize your files.\n",
    "\n",
    "```\n",
    "invoice-parser-project/\n",
    "├── data/\n",
    "│   ├── raw/\n",
    "│   │   ├── sroie_dataset/  # <-- The original 1000 images and labels from SROIE\n",
    "│   │   │   ├── images/\n",
    "│   │   │   │   ├── X51005200619.jpg\n",
    "│   │   │   │   └── ...\n",
    "│   │   │   └── labels/\n",
    "│   │   │       ├── X51005200619.json\n",
    "│   │   │       └── ...\n",
    "│   │   └── new_invoices/     # <-- Your newly collected images\n",
    "│   │       ├── images/\n",
    "│   │       │   ├── invoice_001.png\n",
    "│   │       │   ├── invoice_002.jpg\n",
    "│   │       │   └── ...\n",
    "│   │       └── labels/         # <-- You will create these files MANUALLY\n",
    "│   │           ├── invoice_001.json\n",
    "│   │           ├── invoice_002.json\n",
    "│   │           └── ...\n",
    "└── ... (other project folders like src, notebooks, etc.)\n",
    "```\n",
    "\n",
    "**Step 1.3: The \"Hard Part\" - Create Labels for New Images**\n",
    "\n",
    "This is a manual but invaluable process. For every new image you downloaded, you must create its corresponding JSON label file.\n",
    "\n",
    "**The Process:**\n",
    "\n",
    "1.  Open an image, for example, `new_invoices/images/invoice_001.png`.\n",
    "2.  Open a new, empty text file.\n",
    "3.  Look at the image and identify the key pieces of information.\n",
    "4.  Type this information into the text file in the **exact same JSON format** as the SROIE dataset labels. This is **CRITICAL** for consistency. The SROIE dataset uses keys like `company`, `date`, `address`, and `total`. Stick to these.\n",
    "\n",
    "**Example:**\n",
    "Let's say `invoice_001.png` shows:\n",
    "\n",
    "> **Vendor:** \"Creative Solutions Inc.\"\n",
    "> **Date:** \"Oct 25, 2023\"\n",
    "> **Address:** \"123 Tech Park, Silicon Valley, CA\"\n",
    "> **Total Due:** \"$450.00\"\n",
    "\n",
    "You would create a file named `invoice_001.json` and put the following content inside:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"company\": \"Creative Solutions Inc.\",\n",
    "  \"date\": \"Oct 25, 2023\",\n",
    "  \"address\": \"123 Tech Park, Silicon Valley, CA\",\n",
    "  \"total\": \"450.00\"\n",
    "}\n",
    "```\n",
    "*Notice I removed the \"$\" from the total. It's good practice to standardize your labels. Decide on a rule (e.g., \"always remove currency symbols\") and stick to it.*\n",
    "\n",
    "**Repeat this for all 150-250 new images.** Yes, this is tedious. But it is exactly the kind of \"dirty work\" that Data Scientists do and that hiring managers want to see you can handle.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. When to Split: Train/Validation/Test Split\n",
    "\n",
    "This is a fundamentally important question that separates junior and senior practitioners.\n",
    "\n",
    "**The Golden Rule: Split your dataset *BEFORE* any processing (like OCR).**\n",
    "\n",
    "Your test set must be a sacred, held-out set that simulates truly \"new\" data the model has never seen. If any information from the test set \"leaks\" into your training process, your evaluation metrics will be artificially inflated and untrustworthy.\n",
    "\n",
    "**Here is the Correct Workflow:**\n",
    "\n",
    "1.  **Combine File Lists:** Create a single list of all your image file paths (the 1000 from SROIE + the 250 you added). You should have 1250 images in total.\n",
    "\n",
    "2.  **Split the File List:** Use a script to split this list of filenames into three sets: `train`, `validation`, and `test`. A standard split is 80% train, 10% validation, 10% test.\n",
    "    *   **Train Set (e.g., 1000 images):** Used to train the model.\n",
    "    *   **Validation Set (e.g., 125 images):** Used to tune hyperparameters and check for overfitting during training.\n",
    "    *   **Test Set (e.g., 125 images):** Used *only once* at the very end to evaluate the final model's performance on unseen data.\n",
    "\n",
    "3.  **Physically Segregate Files:** Move the image and its corresponding label file into new folders based on the split.\n",
    "\n",
    "Your `data` folder should now look like this:\n",
    "\n",
    "```\n",
    "data/\n",
    "├── processed/\n",
    "│   ├── train/\n",
    "│   │   ├── images/  (1000 images)\n",
    "│   │   └── labels/  (1000 json files)\n",
    "│   ├── val/\n",
    "│   │   ├── images/  (125 images)\n",
    "│   │   └── labels/  (125 json files)\n",
    "│   └── test/\n",
    "│       ├── images/  (125 images)\n",
    "│       └── labels/  (125 json files)\n",
    "└── raw/\n",
    "    └── ... (your original raw data)\n",
    "```\n",
    "\n",
    "**Why is this the right way?**\n",
    "By splitting first, you ensure that any decisions you make during the OCR and annotation steps (Step 2 & 3) are based *only* on the training data. For example, if you have to write a special rule to help your OCR tool read a weird font, you will discover that need by looking at a *training* image, not a *test* image. This prevents your own human bias from leaking knowledge about the test set into the model.\n",
    "\n",
    "**Now, you are ready for Step 2 (Run OCR). You will run OCR separately on the images in your `train`, `val`, and `test` folders.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Consolidated Action Plan for Step 1**\n",
    "\n",
    "1.  **Gather:** Download ~200 diverse invoices from Google Images.\n",
    "2.  **Organize:** Create the `data/raw/new_invoices/images` folder structure and save them there.\n",
    "3.  **Label:** Manually create a ground-truth `.json` file for each new image you downloaded. Store them in `data/raw/new_invoices/labels`. Ensure the keys (`company`, `date`, etc.) are consistent with the SROIE dataset.\n",
    "4.  **List & Split:**\n",
    "    *   Create a master list of all 1200+ image paths.\n",
    "    *   Use a Python script with `sklearn.model_selection.train_test_split` to divide the list into training, validation, and testing sets.\n",
    "5.  **Segregate:** Write another script to copy the files (both `image.jpg` and `image.json`) into the final `data/processed/train`, `data/processed/val`, and `data/processed/test` directories.\n",
    "\n",
    "You have now successfully completed the most labor-intensive and most important step of your project. The organized and segregated dataset you have at the end of this process is the bedrock of your entire project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e200ebf4",
   "metadata": {},
   "source": [
    "Excellent! You've completed the most complex prerequisite. The C++ compiler is now installed and ready to be used.\n",
    "\n",
    "Now we can proceed with the original plan to install detectron2 and finally run the training script.\n",
    "\n",
    "Here is your clear, step-by-step action plan.\n",
    "\n",
    "Step 1: Prepare Your Terminal\n",
    "\n",
    "Open a new terminal window. This is important to ensure it recognizes the newly installed Build Tools. A regular Command Prompt or PowerShell should work fine.\n",
    "\n",
    "Navigate to your project directory:\n",
    "\n",
    "Generated bash\n",
    "cd D:\\invoice-parser-mlops\n",
    "\n",
    "\n",
    "Activate your Python virtual environment:\n",
    "\n",
    "Generated bash\n",
    "venv\\Scripts\\activate\n",
    "IGNORE_WHEN_COPYING_START\n",
    "content_copy\n",
    "download\n",
    "Use code with caution.\n",
    "Bash\n",
    "IGNORE_WHEN_COPYING_END\n",
    "\n",
    "You should see (env) at the start of your command line.\n",
    "\n",
    "Step 2: Install PyTorch and Other Dependencies\n",
    "\n",
    "We need to install PyTorch, cython, and fvcore before we can install detectron2.\n",
    "\n",
    "Run the following command. Choose the one that matches your system (CPU or GPU).\n",
    "\n",
    "If you are using CPU ONLY:\n",
    "\n",
    "Generated bash\n",
    "pip install torch torchvision torchaudio cython fvcore\n",
    "IGNORE_WHEN_COPYING_START\n",
    "content_copy\n",
    "download\n",
    "Use code with caution.\n",
    "Bash\n",
    "IGNORE_WHEN_COPYING_END\n",
    "\n",
    "If you have an NVIDIA GPU (recommended for speed): Use the command for your specific CUDA version. CUDA 11.8 is very common. You can find the right command on the PyTorch website.\n",
    "\n",
    "Generated bash\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 cython fvcore\n",
    "IGNORE_WHEN_COPYING_START\n",
    "content_copy\n",
    "download\n",
    "Use code with caution.\n",
    "Bash\n",
    "IGNORE_WHEN_COPYING_END\n",
    "Step 3: Install detectron2 (The Main Step)\n",
    "\n",
    "Now we will install detectron2 itself. This command uses the -f flag to tell pip to look for the correct pre-compiled version from Facebook's servers. This might take a few minutes as it may compile some code using the Build Tools you just installed.\n",
    "\n",
    "Again, choose the command that matches your system.\n",
    "\n",
    "If you are using CPU ONLY:\n",
    "\n",
    "Generated bash\n",
    "pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch2.1/index.html\n",
    "IGNORE_WHEN_COPYING_START\n",
    "content_copy\n",
    "download\n",
    "Use code with caution.\n",
    "Bash\n",
    "IGNORE_WHEN_COPYING_END\n",
    "\n",
    "If you have an NVIDIA GPU (with CUDA 11.8):\n",
    "\n",
    "Generated bash\n",
    "pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu118/torch2.1/index.html\n",
    "IGNORE_WHEN_COPYING_START\n",
    "content_copy\n",
    "download\n",
    "Use code with caution.\n",
    "Bash\n",
    "IGNORE_WHEN_COPYING_END\n",
    "\n",
    "If the command completes without any red error messages, detectron2 has been successfully installed.\n",
    "\n",
    "Step 4: Verify the Installation (Quick Test)\n",
    "\n",
    "Before running the full training script, let's do a quick check to make sure the library is installed correctly. With your (env) still active, run this command:\n",
    "\n",
    "Generated bash\n",
    "python -c \"import detectron2; print('Detectron2 installation successful!')\"\n",
    "IGNORE_WHEN_COPYING_START\n",
    "content_copy\n",
    "download\n",
    "Use code with caution.\n",
    "Bash\n",
    "IGNORE_WHEN_COPYING_END\n",
    "\n",
    "If you see the success message, you are ready to go!\n",
    "\n",
    "Step 5: Run the Training Script Again\n",
    "\n",
    "You are now ready to re-run the original training script. The environment is now complete.\n",
    "\n",
    "Generated bash\n",
    "python src/models/train_model.py\n",
    "IGNORE_WHEN_COPYING_START\n",
    "content_copy\n",
    "download\n",
    "Use code with caution.\n",
    "Bash\n",
    "IGNORE_WHEN_COPYING_END\n",
    "\n",
    "This time, the script should get past the ImportError and begin the data preprocessing and model training steps. You will see progress bars from the .map() function and then the training log from the Trainer.\n",
    "\n",
    "What to do if it fails?\n",
    "If pip install detectron2 fails with a compiler error, your first troubleshooting step is to use the Developer Command Prompt.\n",
    "\n",
    "Close your current terminal.\n",
    "\n",
    "Open the \"Developer Command Prompt for VS 2022\" from your Start Menu.\n",
    "\n",
    "Repeat Steps 1, 3, 4, and 5 inside this special prompt. It guarantees a perfectly configured environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65cf3fa",
   "metadata": {},
   "source": [
    "Of course. Let's break down each item on that checklist into a detailed, step-by-step guide with explanations. Follow this exactly, and you will get your training started correctly.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Checklist Item 1: Install Correct PyTorch**\n",
    "\n",
    "**The Goal:** Install the specific version of the PyTorch library that is built to work with your NVIDIA GPU and its CUDA drivers. A standard `pip install torch` will likely install the CPU-only version, which would ignore your RTX 3050 completely.\n",
    "\n",
    "**The \"Why\":** Your `nvidia-smi` output shows you have **CUDA 12.5** drivers. The PyTorch team provides pre-compiled versions for specific CUDA versions. We will use the `cu121` (CUDA 12.1) version, which is the current stable release and is fully compatible with your newer drivers.\n",
    "\n",
    "**The \"How\" (Step-by-Step Commands):**\n",
    "\n",
    "1.  **Open your terminal** or command prompt (like Anaconda Prompt or Windows Terminal) where you manage your Python project environment.\n",
    "\n",
    "2.  **First, completely remove any old versions of PyTorch.** This prevents conflicts.\n",
    "    ```bash\n",
    "    pip uninstall torch torchvision torchaudio\n",
    "    ```\n",
    "    If it asks for confirmation (y/n), type `y` and press Enter. If it says the packages are not found, that's fine too.\n",
    "\n",
    "3.  **Now, install the correct version directly from the official PyTorch download source.** Copy and paste this exact command into your terminal and press Enter.\n",
    "    ```bash\n",
    "    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    ```\n",
    "    This command tells pip to use the special download index for CUDA 12.1 builds. This will be a large download.\n",
    "\n",
    "4.  **Verification:** After the installation is complete, you must verify that PyTorch can see your GPU.\n",
    "    *   Open a Python interpreter by typing `python` in your terminal.\n",
    "    *   Run the following two lines of code:\n",
    "        ```python\n",
    "        import torch\n",
    "        print(torch.cuda.is_available())\n",
    "        ```\n",
    "    *   **The expected output is `True`**. If you see `True`, you have succeeded. If you see `False`, something went wrong in the installation, and you should repeat steps 2 and 3 carefully.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Checklist Item 2: Install Detectron2**\n",
    "\n",
    "**The Goal:** Install `detectron2`, a complex computer vision library from Facebook AI Research that LayoutLMv2 uses for its image processing backbone.\n",
    "\n",
    "**The \"Why\":** Standard `pip install detectron2` often fails because it requires C++ compilation on your machine. The method below installs it directly from its source code repository, which is the most reliable way and will use the C++ build tools you installed earlier.\n",
    "\n",
    "**The \"How\" (Step-by-Step Commands):**\n",
    "\n",
    "1.  **In the same terminal**, run the following command. The quotes are important.\n",
    "    ```bash\n",
    "    python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
    "    ```\n",
    "    You will see a lot of text as it downloads the repository and compiles the code. This might take a few minutes.\n",
    "\n",
    "2.  **Verification:** To confirm it's installed correctly, run a simple import test.\n",
    "    ```bash\n",
    "    python -c \"import detectron2\"\n",
    "    ```\n",
    "    If this command runs and simply returns to the prompt without any errors, the installation was successful.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Checklist Item 3: Update `TrainingArguments`**\n",
    "\n",
    "**The Goal:** Modify your `train.py` script to use settings that are specifically tailored to your 4GB VRAM GPU, preventing memory overload errors.\n",
    "\n",
    "**The \"Why\":** The default settings would immediately crash your GPU. We are using a combination of four techniques: a tiny batch size, gradient accumulation (to compensate), mixed-precision (`fp16`), and gradient checkpointing to aggressively conserve memory.\n",
    "\n",
    "**The \"How\" (Step-by-Step):**\n",
    "\n",
    "1.  **Open your `train.py` file** in your code editor (like VS Code, PyCharm, etc.).\n",
    "\n",
    "2.  **Find the section** that defines `training_args`. It will look something like this:\n",
    "    ```python\n",
    "    # --- Training Arguments ---\n",
    "    training_args = TrainingArguments(\n",
    "        # ... old settings here ...\n",
    "    )\n",
    "    ```\n",
    "\n",
    "3.  **Delete that entire `TrainingArguments` block.**\n",
    "\n",
    "4.  **Copy and paste the following \"AGGRESSIVELY OPTIMIZED\" block** into its place.\n",
    "\n",
    "    ```python\n",
    "    # --- Training Arguments (AGGRESSIVELY OPTIMIZED for 4GB VRAM) ---\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(MODEL_OUTPUT_DIR),\n",
    "        num_train_epochs=5,  # 5 epochs is a good starting point\n",
    "\n",
    "        # --- CRITICAL MEMORY SETTINGS ---\n",
    "        per_device_train_batch_size=1,  # MUST be 1 for a 4GB card. No exceptions.\n",
    "        per_device_eval_batch_size=2,   # Evaluation can often use a slightly larger batch size.\n",
    "        \n",
    "        gradient_accumulation_steps=8,  # VERY IMPORTANT. This will accumulate gradients over 8 steps.\n",
    "                                        # Effective batch size = 1 * 8 = 8.\n",
    "                                        # This stabilizes training without using more VRAM.\n",
    "\n",
    "        fp16=True,                      # Enables mixed-precision. Your most powerful memory-saving tool.\n",
    "        \n",
    "        gradient_checkpointing=True,    # A second memory-saving trick. It trades compute time\n",
    "                                        # (slower training) for less memory usage by not storing\n",
    "                                        # all intermediate activations. PERFECT for your situation.\n",
    "\n",
    "        # --- General Settings ---\n",
    "        learning_rate=3e-5,\n",
    "        logging_steps=50,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        save_total_limit=1,             # Only keep the best model to save disk space.\n",
    "    )\n",
    "    ```\n",
    "\n",
    "5.  **Save the `train.py` file.**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Checklist Item 4: Perform Dry Run**\n",
    "\n",
    "**The Goal:** Run the entire training pipeline on a tiny amount of data to ensure all your settings, paths, and libraries are working together correctly before committing to a multi-hour training session.\n",
    "\n",
    "**The \"Why\":** This is your final sanity check. It's better to find an error in 60 seconds than after waiting 2 hours for the script to crash.\n",
    "\n",
    "**The \"How\" (Step-by-Step):**\n",
    "\n",
    "1.  **Open your `train.py` file again.**\n",
    "\n",
    "2.  **Find the line** where you load your dataset:\n",
    "    ```python\n",
    "    dataset = load_from_disk(PROCESSED_DATA_DIR / \"sroie_dataset_for_layoutlm\")\n",
    "    ```\n",
    "\n",
    "3.  **Immediately after that line, add these two lines of code:**\n",
    "    ```python\n",
    "    # --- TEMPORARY DRY RUN CODE ---\n",
    "    logging.warning(\"PERFORMING DRY RUN ON A SMALL SUBSET (32 samples)\")\n",
    "    dataset[\"train\"] = dataset[\"train\"].select(range(32))\n",
    "    dataset[\"validation\"] = dataset[\"validation\"].select(range(16))\n",
    "    dataset[\"test\"] = dataset[\"test\"].select(range(16))\n",
    "    # --- END DRY RUN CODE ---\n",
    "    ```\n",
    "    This tells the script to only use the first 32 examples for training and 16 for validation/testing.\n",
    "\n",
    "4.  **In a separate terminal**, start the GPU monitor. This will let you watch your VRAM usage in real-time.\n",
    "    ```bash\n",
    "    watch -n 1 nvidia-smi\n",
    "    ```\n",
    "\n",
    "5.  **Now, in your main project terminal, run the training script:**\n",
    "    ```bash\n",
    "    python train.py\n",
    "    ```\n",
    "\n",
    "6.  **Verification:** Watch both terminals.\n",
    "    *   In the `nvidia-smi` terminal, you should see the `Memory-Usage` for your GPU jump up to around `3500MiB / 4096MiB` or higher.\n",
    "    *   In the main terminal, you should see the script start processing the data and then begin training, printing out logs like `Epoch [1/5], Step [1/32]...`.\n",
    "    *   **The most important thing is that you do not get a `CUDA out of memory` error.**\n",
    "    *   Once you see the first few training steps complete successfully, you can stop the script by pressing `Ctrl+C`. The dry run is a success.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Checklist Item 5: Execute Full Training**\n",
    "\n",
    "**The Goal:** Run the script on your entire dataset to produce your final, fine-tuned model.\n",
    "\n",
    "**The \"Why\":** The dry run confirmed your setup is correct. Now it's time for the real work.\n",
    "\n",
    "**The \"How\" (Step-by-Step):**\n",
    "\n",
    "1.  **Open `train.py` one last time.**\n",
    "2.  **CRITICAL: DELETE or comment out the temporary dry run code you added in the previous step.** If you forget this, you will only train on 32 samples!\n",
    "    ```python\n",
    "    # DELETE THESE LINES:\n",
    "    # logging.warning(\"PERFORMING DRY RUN ON A SMALL SUBSET (32 samples)\")\n",
    "    # dataset[\"train\"] = dataset[\"train\"].select(range(32))\n",
    "    # dataset[\"validation\"] = dataset[\"validation\"].select(range(16))\n",
    "    # dataset[\"test\"] = dataset[\"test\"].select(range(16))\n",
    "    ```\n",
    "\n",
    "3.  **Save the file.**\n",
    "\n",
    "4.  **Go to your terminal and start the full training run.** Keep the `watch -n 1 nvidia-smi` window open if you want to monitor it.\n",
    "    ```bash\n",
    "    python train.py\n",
    "    ```\n",
    "\n",
    "5.  **Be patient.** With these memory-saving settings, training will be slower but steady. It could take several hours. Go get a coffee; your GPU is doing the heavy lifting now. The script will save the best model at the end and run a final evaluation on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf442cb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
